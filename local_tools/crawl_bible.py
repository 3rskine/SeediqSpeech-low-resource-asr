import requests
from bs4 import BeautifulSoup
import time
import re
import sys

# --- è¨­å®š ---
# å‰µä¸–ç´€ (Genesis) å…±æœ‰ 50 ç« 
start_chapter = 1
end_chapter = 50
output_file = "seediq_bible_cleaned.txt"
base_url = "https://www.bible.com/bible/3109/GEN.{}.STGDAYA"

# Headers å½è£æˆç€è¦½å™¨ï¼Œé¿å…è¢«æ“‹
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

def clean_text_for_asr(text):
    """
    å°‡è–ç¶“æ–‡å­—è½‰æ›ç‚º ASR è¨“ç·´æ ¼å¼ï¼š
    1. å»é™¤ç« ç¯€æ•¸å­— (1, 2...)
    2. å»é™¤æ¨™é»ç¬¦è™Ÿ
    3. å¼·åˆ¶è½‰å¤§å¯« (é…åˆä½ çš„ Tokenizer Hack)
    4. å»é™¤å¤šé¤˜ç©ºç™½
    """
    # ç§»é™¤æ•¸å­— (ä¾‹å¦‚å¥é¦–çš„ "1 ")
    text = re.sub(r'\d+', '', text)
    
    # ç§»é™¤æ¨™é»ç¬¦è™Ÿ (ä¿ç•™ç©ºæ ¼å’Œå­—æ¯)
    # è³½å¾·å…‹èªå¯èƒ½æœ‰ '- (é€£å­—è™Ÿ)ï¼Œè¦–æƒ…æ³ä¿ç•™æˆ–å»é™¤ï¼Œé€™è£¡å…ˆå»é™¤ä»¥æ±‚ä¿éšª
    text = re.sub(r'[^\w\s]', ' ', text)
    
    # å¼·åˆ¶è½‰å¤§å¯«
    text = text.upper()
    
    # ç¸®æ¸›å¤šé¤˜ç©ºç™½ (æŠŠå¤šå€‹ç©ºç™½è®Šä¸€å€‹)
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

print(f"ğŸš€ é–‹å§‹çˆ¬å–è³½å¾·å…‹èªè–ç¶“ (GEN 1-{end_chapter})...")
print(f"ğŸ’¾ ç›®æ¨™æª”æ¡ˆ: {output_file}")

with open(output_file, "w", encoding="utf-8") as f:
    for chapter in range(start_chapter, end_chapter + 1):
        url = base_url.format(chapter)
        print(f"ğŸ“– æ­£åœ¨è™•ç†ç¬¬ {chapter} ç« ...", end="")
        
        try:
            response = requests.get(url, headers=headers)
            if response.status_code != 200:
                print(f"âŒ å¤±æ•— (Status: {response.status_code})")
                continue
            
            soup = BeautifulSoup(response.text, "html.parser")
            
            # Bible.com çš„ç¶“æ–‡é€šå¸¸åœ¨ class åŒ…å« 'ChapterContent_chapter' çš„ div è£¡
            # è£¡é¢çš„æ¯ä¸€ç¯€é€šå¸¸æ˜¯ span
            # æœ€æš´åŠ›çš„è§£æ³•ï¼šç›´æ¥æŠ“æ‰€æœ‰æ–‡å­—ï¼Œç„¶å¾Œæ¸…æ´—
            content_div = soup.find("div", class_=lambda x: x and "ChapterContent_chapter" in x)
            
            if not content_div:
                # å‚™ç”¨æ–¹æ¡ˆï¼šå¦‚æœæ”¹ç‰ˆäº†ï¼ŒæŠ“æ‰€æœ‰ class ç‚º content çš„
                content_div = soup.find("div", class_="yv-bible-text")

            if content_div:
                raw_text = content_div.get_text(separator=" ")
                cleaned_line = clean_text_for_asr(raw_text)
                
                # å¯«å…¥æª”æ¡ˆ (ä¸€è¡Œä¸€ç« ï¼Œæˆ–è€…ä½ å¯ä»¥å†åˆ‡æ›´ç´°)
                # é€™è£¡ç‚ºäº†è®“ LM å­¸åˆ°æ›´å¤šé€£è²«æ€§ï¼Œæˆ‘å€‘æŠŠæ•´ç« å¯«æˆä¸€è¡Œï¼Œæˆ–è€…ä¾æ“šå¥è™Ÿåˆ‡åˆ†
                # æ—¢ç„¶å·²ç¶“å»æ‰äº†æ¨™é»ï¼Œæˆ‘å€‘å°±æ•´ç« å¯«å…¥
                if cleaned_line:
                    f.write(cleaned_line + "\n")
                    print(f"âœ… æˆåŠŸ (é•·åº¦: {len(cleaned_line)})")
                else:
                    print("âš ï¸ æŠ“ä¸åˆ°å…§å®¹")
            else:
                print("âŒ æ‰¾ä¸åˆ°ç¶“æ–‡å®¹å™¨ (ç¶²ç«™çµæ§‹å¯èƒ½æ”¹è®Š)")

        except Exception as e:
            print(f"âŒ éŒ¯èª¤: {e}")
        
        # ç¦®è²Œæ€§å»¶é²ï¼Œé¿å…è¢«é– IP
        time.sleep(1)

print("-" * 30)
print("ğŸ‰ çˆ¬å–å®Œæˆï¼è«‹æª¢æŸ¥ seediq_bible_cleaned.txt")
